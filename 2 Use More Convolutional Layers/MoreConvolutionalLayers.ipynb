{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred weights for layer: separable_conv2d_20\n",
      "Transferred weights for layer: batch_normalization_18\n",
      "Transferred weights for layer: leaky_re_lu_18\n",
      "Transferred weights for layer: separable_conv2d_21\n",
      "Transferred weights for layer: batch_normalization_19\n",
      "Transferred weights for layer: leaky_re_lu_19\n",
      "Transferred weights for layer: separable_conv2d_22\n",
      "Transferred weights for layer: batch_normalization_20\n",
      "Transferred weights for layer: max_pooling2d_8\n",
      "Transferred weights for layer: spatial_dropout2d_8\n",
      "Transferred weights for layer: leaky_re_lu_20\n",
      "Transferred weights for layer: separable_conv2d_23\n",
      "Transferred weights for layer: batch_normalization_21\n",
      "Transferred weights for layer: leaky_re_lu_21\n",
      "Transferred weights for layer: separable_conv2d_24\n",
      "Transferred weights for layer: batch_normalization_22\n",
      "Transferred weights for layer: leaky_re_lu_22\n",
      "Transferred weights for layer: separable_conv2d_25\n",
      "Transferred weights for layer: batch_normalization_23\n",
      "Transferred weights for layer: max_pooling2d_9\n",
      "Transferred weights for layer: spatial_dropout2d_9\n",
      "Transferred weights for layer: leaky_re_lu_23\n",
      "Transferred weights for layer: separable_conv2d_26\n",
      "Transferred weights for layer: batch_normalization_24\n",
      "Transferred weights for layer: leaky_re_lu_24\n",
      "Transferred weights for layer: separable_conv2d_27\n",
      "Transferred weights for layer: batch_normalization_25\n",
      "Transferred weights for layer: max_pooling2d_10\n",
      "Transferred weights for layer: spatial_dropout2d_10\n",
      "Transferred weights for layer: leaky_re_lu_25\n",
      "Transferred weights for layer: separable_conv2d_28\n",
      "Transferred weights for layer: batch_normalization_26\n",
      "Transferred weights for layer: max_pooling2d_11\n",
      "Transferred weights for layer: spatial_dropout2d_11\n",
      "Transferred weights for layer: leaky_re_lu_26\n",
      "Transferred weights for layer: separable_conv2d_29\n",
      "Transferred weights for layer: global_average_pooling2d_2\n",
      "Epoch 1/1000\n",
      "173/173 [==============================] - 15s 74ms/step - loss: 16.3133 - accuracy: 0.1571 - val_loss: 22.9313 - val_accuracy: 0.1144 - lr: 1.0000e-06\n",
      "Epoch 2/1000\n",
      "173/173 [==============================] - 12s 68ms/step - loss: 15.5564 - accuracy: 0.1542 - val_loss: 15.9640 - val_accuracy: 0.1144 - lr: 2.0800e-05\n",
      "Epoch 3/1000\n",
      "173/173 [==============================] - 12s 68ms/step - loss: 13.2631 - accuracy: 0.1582 - val_loss: 12.2551 - val_accuracy: 0.1242 - lr: 4.0600e-05\n",
      "Epoch 4/1000\n",
      "173/173 [==============================] - 12s 71ms/step - loss: 10.1513 - accuracy: 0.1546 - val_loss: 8.6574 - val_accuracy: 0.1176 - lr: 6.0400e-05\n",
      "Epoch 5/1000\n",
      "173/173 [==============================] - 12s 71ms/step - loss: 7.6704 - accuracy: 0.1731 - val_loss: 6.0158 - val_accuracy: 0.1601 - lr: 8.0200e-05\n",
      "Epoch 6/1000\n",
      "173/173 [==============================] - 12s 71ms/step - loss: 5.9492 - accuracy: 0.2292 - val_loss: 4.7486 - val_accuracy: 0.2092 - lr: 8.0200e-05\n",
      "Epoch 7/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 4.9600 - accuracy: 0.2799 - val_loss: 3.9176 - val_accuracy: 0.2778 - lr: 8.0200e-05\n",
      "Epoch 8/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 4.3895 - accuracy: 0.2922 - val_loss: 3.3629 - val_accuracy: 0.3137 - lr: 8.0200e-05\n",
      "Epoch 9/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 3.8599 - accuracy: 0.3233 - val_loss: 2.9210 - val_accuracy: 0.3431 - lr: 8.0200e-05\n",
      "Epoch 10/1000\n",
      "173/173 [==============================] - 12s 72ms/step - loss: 3.4519 - accuracy: 0.3458 - val_loss: 2.6578 - val_accuracy: 0.3529 - lr: 8.0200e-05\n",
      "Epoch 11/1000\n",
      "173/173 [==============================] - 12s 72ms/step - loss: 3.2603 - accuracy: 0.3541 - val_loss: 2.4025 - val_accuracy: 0.3889 - lr: 8.0200e-05\n",
      "Epoch 12/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 2.9216 - accuracy: 0.3682 - val_loss: 2.1905 - val_accuracy: 0.4248 - lr: 8.0200e-05\n",
      "Epoch 13/1000\n",
      "173/173 [==============================] - 13s 72ms/step - loss: 2.7941 - accuracy: 0.3736 - val_loss: 2.0195 - val_accuracy: 0.4510 - lr: 8.0200e-05\n",
      "Epoch 14/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 2.4899 - accuracy: 0.3979 - val_loss: 1.8991 - val_accuracy: 0.4641 - lr: 8.0200e-05\n",
      "Epoch 15/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 2.4602 - accuracy: 0.3910 - val_loss: 1.7762 - val_accuracy: 0.4837 - lr: 8.0200e-05\n",
      "Epoch 16/1000\n",
      "173/173 [==============================] - 13s 72ms/step - loss: 2.2841 - accuracy: 0.4062 - val_loss: 1.6957 - val_accuracy: 0.4935 - lr: 8.0200e-05\n",
      "Epoch 17/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 2.1880 - accuracy: 0.4088 - val_loss: 1.6419 - val_accuracy: 0.4902 - lr: 8.0200e-05\n",
      "Epoch 18/1000\n",
      "173/173 [==============================] - 13s 76ms/step - loss: 2.0926 - accuracy: 0.4251 - val_loss: 1.5931 - val_accuracy: 0.5065 - lr: 8.0200e-05\n",
      "Epoch 19/1000\n",
      "173/173 [==============================] - 13s 75ms/step - loss: 2.0594 - accuracy: 0.4254 - val_loss: 1.5498 - val_accuracy: 0.5131 - lr: 8.0200e-05\n",
      "Epoch 20/1000\n",
      "173/173 [==============================] - 12s 72ms/step - loss: 1.9454 - accuracy: 0.4421 - val_loss: 1.5000 - val_accuracy: 0.5163 - lr: 8.0200e-05\n",
      "Epoch 21/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.9400 - accuracy: 0.4421 - val_loss: 1.4694 - val_accuracy: 0.5261 - lr: 8.0200e-05\n",
      "Epoch 22/1000\n",
      "173/173 [==============================] - 13s 72ms/step - loss: 1.8656 - accuracy: 0.4508 - val_loss: 1.4301 - val_accuracy: 0.5294 - lr: 8.0200e-05\n",
      "Epoch 23/1000\n",
      "173/173 [==============================] - 14s 78ms/step - loss: 1.8028 - accuracy: 0.4598 - val_loss: 1.4047 - val_accuracy: 0.5359 - lr: 8.0200e-05\n",
      "Epoch 24/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.7994 - accuracy: 0.4522 - val_loss: 1.3750 - val_accuracy: 0.5392 - lr: 8.0200e-05\n",
      "Epoch 25/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.7272 - accuracy: 0.4678 - val_loss: 1.3658 - val_accuracy: 0.5490 - lr: 8.0200e-05\n",
      "Epoch 26/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.7167 - accuracy: 0.4699 - val_loss: 1.3397 - val_accuracy: 0.5523 - lr: 8.0200e-05\n",
      "Epoch 27/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.6372 - accuracy: 0.4870 - val_loss: 1.3275 - val_accuracy: 0.5686 - lr: 8.0200e-05\n",
      "Epoch 28/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.6502 - accuracy: 0.4902 - val_loss: 1.3099 - val_accuracy: 0.5654 - lr: 8.0200e-05\n",
      "Epoch 29/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.5969 - accuracy: 0.4881 - val_loss: 1.2791 - val_accuracy: 0.5752 - lr: 8.0200e-05\n",
      "Epoch 30/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.5555 - accuracy: 0.5036 - val_loss: 1.2553 - val_accuracy: 0.5752 - lr: 8.0200e-05\n",
      "Epoch 31/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.5537 - accuracy: 0.4989 - val_loss: 1.2459 - val_accuracy: 0.5882 - lr: 8.0200e-05\n",
      "Epoch 32/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.5120 - accuracy: 0.5091 - val_loss: 1.2353 - val_accuracy: 0.5882 - lr: 8.0200e-05\n",
      "Epoch 33/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.5042 - accuracy: 0.5134 - val_loss: 1.2194 - val_accuracy: 0.5850 - lr: 8.0200e-05\n",
      "Epoch 34/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.4804 - accuracy: 0.5138 - val_loss: 1.2151 - val_accuracy: 0.5980 - lr: 8.0200e-05\n",
      "Epoch 35/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.4200 - accuracy: 0.5366 - val_loss: 1.1886 - val_accuracy: 0.6046 - lr: 8.0200e-05\n",
      "Epoch 36/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.4140 - accuracy: 0.5387 - val_loss: 1.1834 - val_accuracy: 0.6078 - lr: 8.0200e-05\n",
      "Epoch 37/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3524 - accuracy: 0.5492 - val_loss: 1.1648 - val_accuracy: 0.6209 - lr: 8.0200e-05\n",
      "Epoch 38/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3950 - accuracy: 0.5358 - val_loss: 1.1627 - val_accuracy: 0.6176 - lr: 8.0200e-05\n",
      "Epoch 39/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3589 - accuracy: 0.5402 - val_loss: 1.1411 - val_accuracy: 0.6144 - lr: 8.0200e-05\n",
      "Epoch 40/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3778 - accuracy: 0.5521 - val_loss: 1.1232 - val_accuracy: 0.6176 - lr: 8.0200e-05\n",
      "Epoch 41/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3047 - accuracy: 0.5547 - val_loss: 1.1093 - val_accuracy: 0.6307 - lr: 8.0200e-05\n",
      "Epoch 42/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.3136 - accuracy: 0.5568 - val_loss: 1.1095 - val_accuracy: 0.6275 - lr: 8.0200e-05\n",
      "Epoch 43/1000\n",
      "173/173 [==============================] - 14s 82ms/step - loss: 1.2794 - accuracy: 0.5695 - val_loss: 1.1088 - val_accuracy: 0.6307 - lr: 8.0200e-05\n",
      "Epoch 44/1000\n",
      "173/173 [==============================] - 15s 85ms/step - loss: 1.2569 - accuracy: 0.5768 - val_loss: 1.0921 - val_accuracy: 0.6275 - lr: 8.0200e-05\n",
      "Epoch 45/1000\n",
      "173/173 [==============================] - 13s 77ms/step - loss: 1.2578 - accuracy: 0.5739 - val_loss: 1.0904 - val_accuracy: 0.6275 - lr: 8.0200e-05\n",
      "Epoch 46/1000\n",
      "173/173 [==============================] - 14s 80ms/step - loss: 1.1925 - accuracy: 0.5988 - val_loss: 1.0731 - val_accuracy: 0.6471 - lr: 8.0200e-05\n",
      "Epoch 47/1000\n",
      "173/173 [==============================] - 14s 81ms/step - loss: 1.2407 - accuracy: 0.5825 - val_loss: 1.0598 - val_accuracy: 0.6405 - lr: 8.0200e-05\n",
      "Epoch 48/1000\n",
      "173/173 [==============================] - 14s 78ms/step - loss: 1.2128 - accuracy: 0.5829 - val_loss: 1.0492 - val_accuracy: 0.6471 - lr: 8.0200e-05\n",
      "Epoch 49/1000\n",
      "173/173 [==============================] - 13s 78ms/step - loss: 1.1843 - accuracy: 0.5920 - val_loss: 1.0409 - val_accuracy: 0.6536 - lr: 8.0200e-05\n",
      "Epoch 50/1000\n",
      "173/173 [==============================] - 14s 81ms/step - loss: 1.1579 - accuracy: 0.5992 - val_loss: 1.0345 - val_accuracy: 0.6438 - lr: 8.0200e-05\n",
      "Epoch 51/1000\n",
      "173/173 [==============================] - 14s 82ms/step - loss: 1.1907 - accuracy: 0.5923 - val_loss: 1.0283 - val_accuracy: 0.6438 - lr: 8.0200e-05\n",
      "Epoch 52/1000\n",
      "173/173 [==============================] - 14s 79ms/step - loss: 1.1571 - accuracy: 0.6003 - val_loss: 1.0230 - val_accuracy: 0.6405 - lr: 8.0200e-05\n",
      "Epoch 53/1000\n",
      "173/173 [==============================] - 14s 78ms/step - loss: 1.1359 - accuracy: 0.6079 - val_loss: 1.0070 - val_accuracy: 0.6634 - lr: 8.0200e-05\n",
      "Epoch 54/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.1299 - accuracy: 0.6072 - val_loss: 0.9896 - val_accuracy: 0.6569 - lr: 8.0200e-05\n",
      "Epoch 55/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.1410 - accuracy: 0.6086 - val_loss: 0.9900 - val_accuracy: 0.6634 - lr: 8.0200e-05\n",
      "Epoch 56/1000\n",
      "173/173 [==============================] - 13s 78ms/step - loss: 1.1108 - accuracy: 0.6217 - val_loss: 0.9831 - val_accuracy: 0.6569 - lr: 8.0200e-05\n",
      "Epoch 57/1000\n",
      "173/173 [==============================] - 13s 78ms/step - loss: 1.0801 - accuracy: 0.6408 - val_loss: 0.9793 - val_accuracy: 0.6536 - lr: 8.0200e-05\n",
      "Epoch 58/1000\n",
      "173/173 [==============================] - 13s 75ms/step - loss: 1.0944 - accuracy: 0.6282 - val_loss: 0.9701 - val_accuracy: 0.6634 - lr: 8.0200e-05\n",
      "Epoch 59/1000\n",
      "173/173 [==============================] - 13s 77ms/step - loss: 1.0761 - accuracy: 0.6249 - val_loss: 0.9703 - val_accuracy: 0.6634 - lr: 4.0100e-05\n",
      "Epoch 60/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.0840 - accuracy: 0.6329 - val_loss: 0.9661 - val_accuracy: 0.6699 - lr: 4.0100e-05\n",
      "Epoch 61/1000\n",
      "173/173 [==============================] - 13s 78ms/step - loss: 1.0384 - accuracy: 0.6318 - val_loss: 0.9642 - val_accuracy: 0.6699 - lr: 4.0100e-05\n",
      "Epoch 62/1000\n",
      "173/173 [==============================] - 13s 77ms/step - loss: 1.0728 - accuracy: 0.6318 - val_loss: 0.9584 - val_accuracy: 0.6699 - lr: 4.0100e-05\n",
      "Epoch 63/1000\n",
      "173/173 [==============================] - 13s 75ms/step - loss: 1.0518 - accuracy: 0.6430 - val_loss: 0.9529 - val_accuracy: 0.6667 - lr: 4.0100e-05\n",
      "Epoch 64/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.0587 - accuracy: 0.6343 - val_loss: 0.9514 - val_accuracy: 0.6699 - lr: 4.0100e-05\n",
      "Epoch 65/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.0243 - accuracy: 0.6343 - val_loss: 0.9493 - val_accuracy: 0.6667 - lr: 4.0100e-05\n",
      "Epoch 66/1000\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 1.0501 - accuracy: 0.6506 - val_loss: 0.9451 - val_accuracy: 0.6699 - lr: 2.0050e-05\n",
      "Epoch 67/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.0050 - accuracy: 0.6470 - val_loss: 0.9459 - val_accuracy: 0.6699 - lr: 2.0050e-05\n",
      "Epoch 68/1000\n",
      "173/173 [==============================] - 13s 75ms/step - loss: 1.0398 - accuracy: 0.6495 - val_loss: 0.9443 - val_accuracy: 0.6634 - lr: 2.0050e-05\n",
      "Epoch 69/1000\n",
      "173/173 [==============================] - 14s 78ms/step - loss: 1.0087 - accuracy: 0.6434 - val_loss: 0.9437 - val_accuracy: 0.6699 - lr: 2.0050e-05\n",
      "Epoch 70/1000\n",
      "173/173 [==============================] - 13s 74ms/step - loss: 1.0210 - accuracy: 0.6470 - val_loss: 0.9402 - val_accuracy: 0.6699 - lr: 2.0050e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, SeparableConv2D, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D, Dense, SpatialDropout2D\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def create_exact_matching_model(input_shape=(48, 48, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    x = SeparableConv2D(48, (3, 3), padding='same', name='separable_conv2d_20')(inputs)\n",
    "    x = BatchNormalization(name='batch_normalization_18')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_18')(x)\n",
    "    x = SeparableConv2D(48, (3, 3), padding='same', name='separable_conv2d_21')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_19')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_19')(x)\n",
    "    x = SeparableConv2D(48, (3, 3), padding='same', name='separable_conv2d_22')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_20')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_8')(x)\n",
    "    x = SpatialDropout2D(0.1, name='spatial_dropout2d_8')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_20')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = SeparableConv2D(96, (3, 3), padding='same', name='separable_conv2d_23')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_21')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_21')(x)\n",
    "    x = SeparableConv2D(96, (3, 3), padding='same', name='separable_conv2d_24')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_22')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_22')(x)\n",
    "    x = SeparableConv2D(96, (3, 3), padding='same', name='separable_conv2d_25')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_23')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_9')(x)\n",
    "    x = SpatialDropout2D(0.1, name='spatial_dropout2d_9')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_23')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = SeparableConv2D(192, (3, 3), padding='same', name='separable_conv2d_26')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_24')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_24')(x)\n",
    "    x = SeparableConv2D(192, (3, 3), padding='same', name='separable_conv2d_27')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_25')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_10')(x)\n",
    "    x = SpatialDropout2D(0.1, name='spatial_dropout2d_10')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_25')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = SeparableConv2D(384, (3, 3), padding='same', name='separable_conv2d_28')(x)\n",
    "    x = BatchNormalization(name='batch_normalization_26')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_11')(x)\n",
    "    x = SpatialDropout2D(0.1, name='spatial_dropout2d_11')(x)\n",
    "    x = LeakyReLU(name='leaky_re_lu_26')(x)\n",
    "\n",
    "    # Final layers\n",
    "    x = SeparableConv2D(\n",
    "        filters=8,\n",
    "        kernel_size=(1, 1),\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "        dilation_rate=(1, 1),\n",
    "        depth_multiplier=1,\n",
    "        activation='linear',\n",
    "        use_bias=True,\n",
    "        depthwise_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "        pointwise_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        name='separable_conv2d_29'\n",
    "    )(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D(name='global_average_pooling2d_2')(x)\n",
    "    outputs = Dense(7, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class WarmupScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, warmup_epochs=5, initial_lr=1e-6, target_lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * (epoch / self.warmup_epochs)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "\n",
    "def custom_normalization(image):\n",
    "    \"\"\"Normalize image using same method as FER+ training\"\"\"\n",
    "    image = image.astype('float32')\n",
    "    image = image / 255.0\n",
    "    image = (image - 0.5) * 2.0\n",
    "    return image\n",
    "\n",
    "def load_and_transfer_weights(base_model_path):\n",
    "    base_model = tf.keras.models.load_model(base_model_path)\n",
    "    new_model = create_exact_matching_model()\n",
    "    \n",
    "    for layer in new_model.layers:\n",
    "        try:\n",
    "            layer_idx = [i for i, l in enumerate(base_model.layers) if l.name == layer.name]\n",
    "            if layer_idx:\n",
    "                layer.set_weights(base_model.layers[layer_idx[0]].get_weights())\n",
    "                print(f\"Transferred weights for layer: {layer.name}\")\n",
    "        except:\n",
    "            print(f\"Could not transfer weights for layer: {layer.name}\")\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "def prepare_dataset(labels_path, dataset_path):\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    demographic_data = []\n",
    "    \n",
    "    for idx, row in labels_df.iterrows():\n",
    "        img_path = os.path.join(dataset_path, str(row['label']), row['image'])\n",
    "        try:\n",
    "            img = load_img(img_path, color_mode='grayscale', target_size=(48, 48))\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = custom_normalization(img_array)\n",
    "            images.append(img_array)\n",
    "            labels.append(row['label'] - 1)  # Zero-based indexing\n",
    "            demographic_data.append([row['Gender'], row['Age_Group']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "    \n",
    "    X = np.array(images)\n",
    "    y = tf.keras.utils.to_categorical(labels)\n",
    "    demographic_data = np.array(demographic_data)\n",
    "    \n",
    "    return X, y, demographic_data\n",
    "\n",
    "def train_model(base_model_path, labels_path, dataset_path, output_path):\n",
    "    # Load and prepare data\n",
    "    X, y, demographic_data = prepare_dataset(labels_path, dataset_path)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = load_and_transfer_weights(base_model_path)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        WarmupScheduler(),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create train/val split\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    val_size = int(0.1 * len(X))\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=1000,\n",
    "        batch_size=16,  # Reduced batch size\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(output_path)\n",
    "    return model, history\n",
    "\n",
    "# Execute training\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_MODEL_PATH = 'ferplus_model.h5'\n",
    "    LABELS_PATH = 'archive/test_labels.csv'\n",
    "    DATASET_PATH = 'archive/DATASET/test'\n",
    "    OUTPUT_PATH = '2/DenseConvNet.h5'\n",
    "    \n",
    "    model, history = train_model(BASE_MODEL_PATH, LABELS_PATH, DATASET_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 4s 41ms/step\n",
      "\n",
      "Overall Accuracy: 0.7900912646675359\n",
      "\n",
      "Emotion-wise Accuracies:\n",
      "Emotion 0: 0.742\n",
      "Emotion 1: 0.257\n",
      "Emotion 2: 0.231\n",
      "Emotion 3: 0.931\n",
      "Emotion 4: 0.713\n",
      "Emotion 5: 0.599\n",
      "Emotion 6: 0.857\n",
      "\n",
      "Gender Accuracies:\n",
      "Gender 0: 0.782\n",
      "Gender 1: 0.798\n",
      "\n",
      "Age Group Accuracies:\n",
      "Age Group 1: 0.831\n",
      "Age Group 2: 0.771\n",
      "Age Group 3: 0.802\n",
      "Age Group 4: 0.754\n",
      "Age Group 5: 0.708\n",
      "\n",
      "Fairness Scores:\n",
      "Gender Fairness: 0.981\n",
      "Age Fairness: 0.853\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def custom_normalization(image):\n",
    "    \"\"\"Match training normalization\"\"\"\n",
    "    image = image.astype('float32')\n",
    "    image = image / 255.0\n",
    "    image = (image - 0.5) * 2.0\n",
    "    return image\n",
    "\n",
    "def evaluate_model(model_path, test_labels_path, test_data_path):\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load test data with matching normalization\n",
    "    test_df = pd.read_csv(test_labels_path)\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    demo_data = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        img_path = os.path.join(test_data_path, str(row['label']), row['image'])\n",
    "        try:\n",
    "            img = load_img(img_path, color_mode='grayscale', target_size=(48, 48))\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = custom_normalization(img_array)\n",
    "            X_test.append(img_array)\n",
    "            y_test.append(row['label'] - 1)  # Zero-based indexing\n",
    "            demo_data.append([row['Gender'], row['Age_Group']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    demo_data = np.array(demo_data)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_acc = accuracy_score(y_test, y_pred_classes)\n",
    "    \n",
    "    # Per-emotion accuracy\n",
    "    emotion_accs = {}\n",
    "    for i in range(7):\n",
    "        mask = y_test == i\n",
    "        if np.any(mask):\n",
    "            acc = accuracy_score(y_test[mask], y_pred_classes[mask])\n",
    "            emotion_accs[i] = acc\n",
    "    \n",
    "    # Demographic fairness\n",
    "    gender_accs = {}\n",
    "    for gender in [0, 1]:\n",
    "        mask = demo_data[:, 0] == gender\n",
    "        gender_accs[gender] = accuracy_score(y_test[mask], y_pred_classes[mask])\n",
    "    \n",
    "    age_accs = {}\n",
    "    for age in range(1, 6):\n",
    "        mask = demo_data[:, 1] == age\n",
    "        age_accs[age] = accuracy_score(y_test[mask], y_pred_classes[mask])\n",
    "    \n",
    "    return overall_acc, emotion_accs, gender_accs, age_accs\n",
    "\n",
    "# Usage\n",
    "model_path = 'DenseConvNet.h5'\n",
    "test_labels_path = '../archive/test_labels.csv'\n",
    "test_data_path = '../archive/DATASET/test'\n",
    "\n",
    "overall_acc, emotion_accs, gender_accs, age_accs = evaluate_model(\n",
    "    model_path, test_labels_path, test_data_path)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOverall Accuracy:\", overall_acc)\n",
    "\n",
    "print(\"\\nEmotion-wise Accuracies:\")\n",
    "for emotion, acc in emotion_accs.items():\n",
    "    print(f\"Emotion {emotion}: {acc:.3f}\")\n",
    "\n",
    "print(\"\\nGender Accuracies:\")\n",
    "for gender, acc in gender_accs.items():\n",
    "    print(f\"Gender {gender}: {acc:.3f}\")\n",
    "\n",
    "print(\"\\nAge Group Accuracies:\")\n",
    "for age, acc in age_accs.items():\n",
    "    print(f\"Age Group {age}: {acc:.3f}\")\n",
    "\n",
    "# Calculate fairness scores\n",
    "gender_fairness = min(gender_accs.values()) / max(gender_accs.values())\n",
    "age_fairness = min(age_accs.values()) / max(age_accs.values())\n",
    "\n",
    "print(\"\\nFairness Scores:\")\n",
    "print(f\"Gender Fairness: {gender_fairness:.3f}\")\n",
    "print(f\"Age Fairness: {age_fairness:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
